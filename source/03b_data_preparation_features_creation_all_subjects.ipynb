{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c2fe28-c522-47fa-b55b-56ec5979d07d",
   "metadata": {},
   "source": [
    "# 03b - Data Preparation - Features Creation - Run for All Subjects\n",
    "* (1) Create df_acc files for all subjects\n",
    "* (2) Create df_fea filess for all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b69dc3b-030e-4097-83c2-d6bd156a12f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import utils\n",
    "from constants import Constants\n",
    "from joblib import Parallel, delayed\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde67a6-e79a-40ba-a135-cc268642793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "(1) Create feature file in a DataFrame and save to parquet and csv formats\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58ee28da-0359-4cbf-b2a6-583b05b90fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to convert acc dataframe for Subject=id4ea159a8\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to convert acc dataframe for Subject=id7c20ee7a\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to convert acc dataframe for Subject=idbae5a811\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to convert acc dataframe for Subject=idf5e3678b\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to convert acc dataframe for Subject=id8af5374b\n",
      "Completed converting\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "(1a) Converting all df_acc files\n",
    "'''\n",
    "\n",
    "def _convert_df_acc(all_files, file_id):\n",
    "    # Load one accelerometer data file into dataframe \n",
    "    file_id, df = utils.load_single_data(all_files, file_id)\n",
    "    subject_id = file_id.split('/')[-1].split('.')[0]\n",
    "    print('Begin to convert acc dataframe for Subject='+subject_id)\n",
    "    \n",
    "    # General setting\n",
    "    location_id = ['la']\n",
    "    win_length = 6  # default will be 10-sec for each event\n",
    "    win_shift = 3  # No overlapping between each event \n",
    "    acc_inputs = ['la_x', 'la_y', 'la_z']   # Only select Left-Ankle for now\n",
    "    activities = [1, 2, 3, 4, 77, 99]       # All 6 activities with annotations\n",
    "    activities_TP = [1, 2, 3]               # 1=walking; 2=descending stairs; 3=ascending stairs;\n",
    "    activities_TN = [4, 77, 99]             # 4=driving; 77=clapping; 99=non-study activity;\n",
    "    \n",
    "    # Create an empty dataframe with defaulted column names \n",
    "    columns_features = ['subject_id', 'device_loc', 'act_id', 'act_name', 'event_num', 'walk_or_not', 'unique_id', 'time', 'acc_x', 'acc_y', 'acc_z']\n",
    "    df_fea_all = pd.DataFrame(columns=columns_features)\n",
    "    \n",
    "    device_loc = utils.get_device_location_for_df(location_id[0])\n",
    "    \n",
    "    for activity in activities:\n",
    "        # create a temporal dataframe \n",
    "        df_fea_layer_1 = pd.DataFrame(columns=columns_features)\n",
    "        \n",
    "        df_act = df[df.activity==activity]\n",
    "        act_id = activity\n",
    "        act_name = utils.get_activity_type_for_df(activity)\n",
    "    \n",
    "        # Begin to cut data into several 10-sec events\n",
    "        total_segments_sec = round( (df_act.time_s.iloc[-1]-df_act.time_s.iloc[0]) / ( win_length-(win_length-win_shift) ) )  # compute how many segmnets in second will be cut\n",
    "        for count, num in enumerate(range(0, total_segments_sec), start=1):\n",
    "            df_fea_layer_2 = pd.DataFrame(columns=columns_features)\n",
    "            \n",
    "            cut_begin = df_act.time_s.iloc[0] + num*win_length\n",
    "            cut_end   = cut_begin + win_length \n",
    "            df_cut_tmp = df_act[(df_act.time_s>=cut_begin) & (df_act.time_s<=cut_end)]\n",
    "            \n",
    "            # Write values into a temporal feature dataframe\n",
    "            if df_cut_tmp.empty:\n",
    "                df_fea_layer_2 = pd.DataFrame(columns=columns_features)  # If no values within the cut datafrmae\n",
    "            else:\n",
    "                if (df_cut_tmp.time_s.iloc[-1]-df_cut_tmp.time_s.iloc[0]) < win_length*0.9: \n",
    "                    # apply quality control to make sure the segment does not include missing data\n",
    "                    # If the segment length is less than 90% of the target time window (9 sec in this case), discard this segment. \n",
    "                    df_fea_layer_2 = pd.DataFrame(columns=columns_features)\n",
    "                else:\n",
    "                    # Write the cut dataframe into a temporal feature dataframe\n",
    "                    df_fea_layer_2['time'] = df_cut_tmp['time_s']\n",
    "                    df_fea_layer_2['acc_x'] = df_cut_tmp['la_x']\n",
    "                    df_fea_layer_2['acc_y'] = df_cut_tmp['la_y']\n",
    "                    df_fea_layer_2['acc_z'] = df_cut_tmp['la_z']\n",
    "            \n",
    "                    # Write others values into the temporal feature dataframe \n",
    "                    df_fea_layer_2['subject_id'] = subject_id\n",
    "                    df_fea_layer_2['device_loc'] = device_loc\n",
    "                    df_fea_layer_2['act_id'] = act_id\n",
    "                    df_fea_layer_2['act_name'] = act_name\n",
    "                    df_fea_layer_2['event_num'] = count\n",
    "    \n",
    "                    unique_id_template = subject_id+'_'+device_loc+'_'+str(activity)+'_'+act_name+'_'+str(count)\n",
    "                    if activity in activities_TP:\n",
    "                        df_fea_layer_2['walk_or_not'] = 1\n",
    "                        df_fea_layer_2['unique_id'] = unique_id_template + '_1'\n",
    "                    elif activity in activities_TN:\n",
    "                        df_fea_layer_2['walk_or_not'] = 0\n",
    "                        df_fea_layer_2['unique_id'] = unique_id_template + '_0'\n",
    "                    \n",
    "            # Reset dataframe index\n",
    "            df_fea_layer_2 = df_fea_layer_2.reset_index(drop=True)\n",
    "    \n",
    "            # Concat each 10-sec dataframe into a larger dataframe\n",
    "            df_fea_layer_1 = pd.concat([df_fea_layer_1, df_fea_layer_2])\n",
    "            df_fea_layer_1 = df_fea_layer_1.reset_index(drop=True)\n",
    "            \n",
    "        # Concat all dataframes into one big final dataframe\n",
    "        df_fea_all = pd.concat([df_fea_all, df_fea_layer_1])\n",
    "        df_fea_all = df_fea_all.reset_index(drop=True)    \n",
    "    \n",
    "    # Save the output dataframe to a parquet format (to save up to 90% of space)\n",
    "    df_fea_all.to_parquet('../outputs/df_acc/df_acc_la_'+subject_id+'.parquet')\n",
    "    # df_fea_all.to_csv('../outputs/df_acc/df_acc_la_'+subject_id+'.csv')\n",
    "    print('Completed converting')\n",
    "    print('-------------------------------------------------------')\n",
    "\n",
    "\n",
    "'''\n",
    "Begin to run parallel computing with joblib\n",
    "'''\n",
    "# Get a list of all accelerometer files\n",
    "all_files = utils.get_all_files()\n",
    "\n",
    "# For regular computing process\n",
    "# for file_id in range(len(all_files)):\n",
    "#     _convert(all_files, file_id)\n",
    "for file_id in range(22, len(all_files)):\n",
    "    _convert_df_acc(all_files, file_id)\n",
    "    \n",
    "# # Define a function to perform conversion on a single file\n",
    "# def convert_single_file(all_files, file_id):\n",
    "#     _convert(all_files, file_id)\n",
    "\n",
    "# # Define the list of file IDs to process\n",
    "# file_ids = range(len(all_files))\n",
    "\n",
    "# # Define the number of parallel jobs\n",
    "# num_jobs = -1  # Set to -1 to use all available CPU cores, or specify the number of cores\n",
    "\n",
    "# # Use joblib to parallelize the conversion process\n",
    "# Parallel(n_jobs=num_jobs)(delayed(convert_single_file)(all_files, file_id) for file_id in file_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c66021b-f077-4d91-8ec2-d302b91fbe2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to compute features dataframe file for Subject id8af5374b\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject id079c763c\n",
      "Completed converting\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinchao/anaconda3/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to compute features dataframe file for Subject id86237981\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject id00b70b13\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject id5308a7d6\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject idecc9265e\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject idb221f542\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject idf540d82b\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject id3e3e50c7\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject id1c7e64ad\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject idc735fc09\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject id650857ca\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject idbae5a811\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject idfc5f05e4\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject idf5e3678b\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject id82b9735c\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject id5993bf4a\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject id34e056c8\n",
      "Completed converting\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to compute features dataframe file for Subject id1165e00c\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject id4ea159a8\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject id7c20ee7a\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject idc91a49d0\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject idf1ce9a0f\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject id37a54bbf\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject id8e66893c\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject idff99de96\n",
      "Completed converting\n",
      "-------------------------------------------------------\n",
      "Begin to compute features dataframe file for Subject id687ab496\n",
      "Completed converting\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "(1b) Converting all df_fea files \n",
    "'''\n",
    "\n",
    "def _convert_df_fea(acc_file):\n",
    "    subject_id = acc_file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "    print('Begin to compute features dataframe file for Subject '+subject_id)\n",
    "    \n",
    "    # Load existing df_acc file\n",
    "    df = pd.read_parquet(acc_file)\n",
    "    \n",
    "    # Create a new dataframe to include features\n",
    "    columns_features = ['subject_id', 'device_loc', 'act_id', 'act_name', 'event_num', 'walk_or_not', 'unique_id', 'time', 'acc_x', 'acc_y', 'acc_z', 'smv']\n",
    "    input_data = ['acc_x', 'acc_y', 'acc_z', 'smv']   # Input time series for computing features\n",
    "    features_types = ['mean', 'std', 'min', 'max']    # Features list to compute\n",
    "    features_names_for_df = ['fea_' + input_item + '_' + feature for input_item in input_data for feature in features_types]  # feature names used in a new dataframe \n",
    "    columns_added_features = columns_features + features_names_for_df\n",
    "    df_fea_all = pd.DataFrame(columns=columns_features)\n",
    "        \n",
    "    def compute_features(df_event):\n",
    "        \n",
    "        def _smv(x, y, z):\n",
    "            '''\n",
    "            Compute Signal Mangitude Vector from 3-axis acc data\n",
    "            '''\n",
    "            return np.sqrt((x*x) + (y*y) + (z*z))\n",
    "        \n",
    "        t = df_event['time']\n",
    "        acc_x = df_event['acc_x']\n",
    "        acc_y = df_event['acc_y']\n",
    "        acc_z = df_event['acc_z']\n",
    "        \n",
    "        # Samples quality check: interpolate any null-valued samples\n",
    "        acc_x = utils._interpolate(acc_x)\n",
    "        acc_y = utils._interpolate(acc_y)\n",
    "        acc_z = utils._interpolate(acc_z)\n",
    "        \n",
    "        smv = _smv(acc_x, acc_y, acc_z)\n",
    "        df_event = df_event.copy()\n",
    "        df_event.loc[:, 'smv'] = smv \n",
    "        \n",
    "        # Begin computing features\n",
    "        for input_cmp in input_data:\n",
    "            data = df_event[input_cmp]\n",
    "            for fea_type in features_types:\n",
    "                if fea_type == 'mean':\n",
    "                    df_event = df_event.copy()\n",
    "                    df_event.loc[:, 'fea_'+input_cmp+'_'+fea_type] = np.mean(data)\n",
    "                elif fea_type == 'std':\n",
    "                    df_event = df_event.copy()\n",
    "                    df_event.loc[:, 'fea_'+input_cmp+'_'+fea_type] = np.std(data)\n",
    "                elif fea_type == 'min':\n",
    "                    df_event = df_event.copy()\n",
    "                    df_event.loc[:, 'fea_'+input_cmp+'_'+fea_type] = np.min(data)                \n",
    "                elif fea_type == 'max':\n",
    "                    df_event = df_event.copy()\n",
    "                    df_event.loc[:, 'fea_'+input_cmp+'_'+fea_type] = np.max(data)    \n",
    "                    \n",
    "        return df_event\n",
    "    \n",
    "    \n",
    "    # Begin to compute features for all event\n",
    "    all_unique_events = np.unique(df.unique_id)\n",
    "    \n",
    "    for event in all_unique_events:\n",
    "        df_event = df[df.unique_id==event]  # Cut dataframe into single event\n",
    "    \n",
    "        # Compute features\n",
    "        df_event = compute_features(df_event)\n",
    "    \n",
    "        # Concat each event into a larger dataframe\n",
    "        df_fea_all = pd.concat([df_fea_all, df_event])\n",
    "        df_fea_all = df_fea_all.reset_index(drop=True)\n",
    "\n",
    "    # Save the df_fea file to a parquet format (to save up to 90% of space)\n",
    "    df_fea_all.to_parquet('../outputs/local_df_fea/df_fea_la_'+subject_id+'.parquet')\n",
    "    print('Completed converting')\n",
    "    print('-------------------------------------------------------')\n",
    "\n",
    "\n",
    "# Load all df_acc files\n",
    "df_acc_path = \"../outputs/local_df_acc/\" # Directory of raw accelerometer data \n",
    "all_acc_files = glob.glob(df_acc_path+'*.parquet') \n",
    "\n",
    "# # Run computation in regular for loop\n",
    "# for acc_file in all_acc_files:\n",
    "#     print(acc_file)\n",
    "#     _convert_df_fea(acc_file)\n",
    "\n",
    "# Use joblib parallel computing \n",
    "# Define a function to perform conversion on a single file\n",
    "def convert_single_file(acc_file):\n",
    "    _convert_df_fea(acc_file)\n",
    "\n",
    "# Define the number of parallel jobs\n",
    "num_jobs = -1  # Set to -1 to use all available CPU cores, or specify the number of cores\n",
    "\n",
    "# Use joblib to parallelize the conversion process\n",
    "Parallel(n_jobs=num_jobs)(delayed(convert_single_file)(acc_file) for acc_file in all_acc_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
